# Generative-AI-with-LLM
This course takes me on a deep dive into how LLM technology actually works, including many of the technical details, like model training, instruction tuning, fine-tuning, the generative AI project life cycle framework to help me plan and execute my projects, and so on.

#### **Week-1:** 

**i** Examine the transformer architecture that powers large language models <br>
**ii** Explore how these models are trained, and <br>
**iii** Understand the computing resources required to develop these powerful LLMs <br>
**vi** Learn about a technique called in-context learning <br>
**v** How to guide the model to output at inference time with prompt engineering, and <br>
**vi** how to tune the most important generation parameters of LLMs to tune your model output.

In Week 2,
you'll explore options for adapting pre-trained models to specific tasks and datasets via a process called instruction fine-tuning.
Then, in Week 3,
you'll see how to align the output of language models with human values to increase helpfulness and decrease potential harm and toxicity.

Each week includes a hands-on lab where you can try out these techniques for yourself in an AWS environment that includes all the resources you need for working with large models at no cost to you.
 In the first hands-on lab,
you'll construct a compare different prompts and inputs for a given generative task, in this case, dialogue summarization.
You'll also explore different inference parameters and sampling strategies to gain intuition on how to further improve the generative model of responses.

In the second hands-on lab,
you'll find tune it existing large language model from Hugging Face, a popular open-source model hub.
You'll play with both full fine-tuning and parameter-efficient fine-tuning, or PEFT for short.
You'll see how PEFT lets you make your workflow much more efficient.

In the third lab, 
you get hands-on with reinforcement learning from human feedback or RLHF, you'll build a reward model classifier to label model responses as either toxic or non-toxic.

##### **Course Instructors** <br>
1. Ehsan Kamalinejad, Ph.D. <br>
Machine Learning Applied Scientist, AWS <br>
2. Nashlie Sephus, Ph.D. <br>
Principal Technology Evangelist for Amazon AI, AWS <br>
3.Saleh Soltan, Ph.D. <br>
Senior Applied Scientist, Amazon Alexa <br>
4. Heiko Hotz <br>
Senior Solutions Architect for AI & Machine Learning, AWS <br>



